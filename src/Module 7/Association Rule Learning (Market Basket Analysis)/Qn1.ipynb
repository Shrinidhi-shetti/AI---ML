{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title: Apriori Algorithm\n",
    "\n",
    "# Task 1: Grocery Store Transactions\n",
    "# Dataset: {Milk, Bread}, {Milk, Diaper, Beer, Bread}, {Milk, Diaper, Beer, Coke}, {Bread, Egg, Milk}, {Bread, Egg, Diaper, Milk, Beer}\n",
    "# Task: Identify frequent item sets using the Apriori Algorithm with a minimum support threshold of 50%.\n",
    "\n",
    "# Task 2: Retail Store Data\n",
    "# Dataset: {Shirt, Tie}, {Shirt, Belt, Tie}, {Shirt, Belt}, {Tie, Belt}, {Shirt, Tie, Belt}\n",
    "# Task: Generate association rules after identifying frequent itemsets with a confidence threshold of 60%.\n",
    "\n",
    "# Task 3: Bookstore Purchases\n",
    "# Dataset: {Book A, Book B}, {Book A, Book C}, {Book B, Book C, Book A}, {Book B, Book D}\n",
    "# Task: Use the Apriori algorithm to find rules with a support threshold of 40% and confidence threshold of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets (Task 1 - Grocery Store Transactions):\n",
      "L1: {frozenset({'Bread'}): 4, frozenset({'Milk'}): 5, frozenset({'Beer'}): 3, frozenset({'Diaper'}): 3}\n",
      "L2: {frozenset({'Bread', 'Milk'}): 4, frozenset({'Diaper', 'Milk'}): 3, frozenset({'Beer', 'Diaper'}): 3, frozenset({'Beer', 'Milk'}): 3}\n",
      "L3: {frozenset({'Beer', 'Diaper', 'Milk'}): 3}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def apriori_task1(transactions, min_support):\n",
    "    # Count occurrences of individual items\n",
    "    item_counts = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_counts[item] += 1\n",
    "\n",
    "    num_transactions = len(transactions)\n",
    "\n",
    "    # Generate initial frequent 1-itemsets\n",
    "    frequent_items = {frozenset([item]): count for item, count in item_counts.items() if count / num_transactions >= min_support}\n",
    "    frequent_itemsets = {1: frequent_items}\n",
    "\n",
    "    k = 2\n",
    "    while True:\n",
    "        candidate_itemsets = set()\n",
    "        prev_frequent_itemsets = frequent_itemsets[k - 1]\n",
    "\n",
    "        # Generate candidates for k-itemsets from (k-1)-itemsets\n",
    "        # This part of candidate generation in your original code is a bit off\n",
    "        # It should combine itemsets from the previous level.\n",
    "        # A common way is to join itemsets that share k-2 items.\n",
    "        for itemset1 in prev_frequent_itemsets:\n",
    "            for itemset2 in prev_frequent_itemsets:\n",
    "                union = itemset1.union(itemset2)\n",
    "                if len(union) == k:\n",
    "                    # Pruning step: Ensure all (k-1)-subsets are frequent\n",
    "                    is_valid_candidate = True\n",
    "                    for subset in itertools.combinations(union, k - 1):\n",
    "                        if frozenset(subset) not in prev_frequent_itemsets:\n",
    "                            is_valid_candidate = False\n",
    "                            break\n",
    "                    if is_valid_candidate:\n",
    "                        candidate_itemsets.add(union)\n",
    "\n",
    "        current_frequent_itemsets = {}\n",
    "        for candidate in candidate_itemsets:\n",
    "            count = 0\n",
    "            for transaction in transactions:\n",
    "                if candidate.issubset(transaction):\n",
    "                    count += 1\n",
    "            if count / num_transactions >= min_support:\n",
    "                current_frequent_itemsets[candidate] = count\n",
    "\n",
    "        if not current_frequent_itemsets:\n",
    "            break\n",
    "        frequent_itemsets[k] = current_frequent_itemsets\n",
    "        k += 1\n",
    "\n",
    "    return frequent_itemsets\n",
    "\n",
    "import itertools # Added for combinations\n",
    "\n",
    "transactions_task1 = [\n",
    "    frozenset(['Milk', 'Bread']),\n",
    "    frozenset(['Milk', 'Diaper', 'Beer', 'Bread']),\n",
    "    frozenset(['Milk', 'Diaper', 'Beer', 'Coke']),\n",
    "    frozenset(['Bread', 'Egg', 'Milk']),\n",
    "    frozenset(['Bread', 'Egg', 'Diaper', 'Milk', 'Beer'])\n",
    "]\n",
    "min_support_task1 = 0.5\n",
    "\n",
    "frequent_itemsets_task1 = apriori_task1(transactions_task1, min_support_task1)\n",
    "\n",
    "print(\"Frequent Itemsets (Task 1 - Grocery Store Transactions):\")\n",
    "for k, itemsets in frequent_itemsets_task1.items():\n",
    "    print(f\"L{k}: {itemsets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- One-Hot Encoded DataFrame ---\n",
      "    Belt  Shirt  Shoes   Tie\n",
      "0  False   True  False  True\n",
      "1   True   True  False  True\n",
      "2  False   True   True  True\n",
      "3   True   True  False  True\n",
      "4   True   True  False  True\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Frequent Itemsets (min_support=0.6) ---\n",
      "   support            itemsets\n",
      "0      0.6              (Belt)\n",
      "1      1.0             (Shirt)\n",
      "2      1.0               (Tie)\n",
      "3      0.6       (Shirt, Belt)\n",
      "4      0.6         (Tie, Belt)\n",
      "5      1.0        (Shirt, Tie)\n",
      "6      0.6  (Shirt, Tie, Belt)\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Association Rules (min_threshold=0.7, metric='confidence') ---\n",
      "     antecedents   consequents  antecedent support  consequent support  \\\n",
      "0         (Belt)       (Shirt)                 0.6                 1.0   \n",
      "1         (Belt)         (Tie)                 0.6                 1.0   \n",
      "2        (Shirt)         (Tie)                 1.0                 1.0   \n",
      "3          (Tie)       (Shirt)                 1.0                 1.0   \n",
      "4  (Shirt, Belt)         (Tie)                 0.6                 1.0   \n",
      "5    (Tie, Belt)       (Shirt)                 0.6                 1.0   \n",
      "6         (Belt)  (Shirt, Tie)                 0.6                 1.0   \n",
      "\n",
      "   support  confidence  lift  representativity  leverage  conviction  \\\n",
      "0      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "1      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "2      1.0         1.0   1.0               1.0       0.0         inf   \n",
      "3      1.0         1.0   1.0               1.0       0.0         inf   \n",
      "4      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "5      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "6      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "\n",
      "   zhangs_metric  jaccard  certainty  kulczynski  \n",
      "0            0.0      0.6        0.0         0.8  \n",
      "1            0.0      0.6        0.0         0.8  \n",
      "2            0.0      1.0        0.0         1.0  \n",
      "3            0.0      1.0        0.0         1.0  \n",
      "4            0.0      0.6        0.0         0.8  \n",
      "5            0.0      0.6        0.0         0.8  \n",
      "6            0.0      0.6        0.0         0.8  \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Sorted Rules by Lift (descending) ---\n",
      "     antecedents   consequents  antecedent support  consequent support  \\\n",
      "0         (Belt)       (Shirt)                 0.6                 1.0   \n",
      "1         (Belt)         (Tie)                 0.6                 1.0   \n",
      "2        (Shirt)         (Tie)                 1.0                 1.0   \n",
      "3          (Tie)       (Shirt)                 1.0                 1.0   \n",
      "4  (Shirt, Belt)         (Tie)                 0.6                 1.0   \n",
      "5    (Tie, Belt)       (Shirt)                 0.6                 1.0   \n",
      "6         (Belt)  (Shirt, Tie)                 0.6                 1.0   \n",
      "\n",
      "   support  confidence  lift  representativity  leverage  conviction  \\\n",
      "0      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "1      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "2      1.0         1.0   1.0               1.0       0.0         inf   \n",
      "3      1.0         1.0   1.0               1.0       0.0         inf   \n",
      "4      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "5      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "6      0.6         1.0   1.0               1.0       0.0         inf   \n",
      "\n",
      "   zhangs_metric  jaccard  certainty  kulczynski  \n",
      "0            0.0      0.6        0.0         0.8  \n",
      "1            0.0      0.6        0.0         0.8  \n",
      "2            0.0      1.0        0.0         1.0  \n",
      "3            0.0      1.0        0.0         1.0  \n",
      "4            0.0      0.6        0.0         0.8  \n",
      "5            0.0      0.6        0.0         0.8  \n",
      "6            0.0      0.6        0.0         0.8  \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- Rules with High Confidence and Lift ---\n",
      "Empty DataFrame\n",
      "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, representativity, leverage, conviction, zhangs_metric, jaccard, certainty, kulczynski]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/mlxtend/frequent_patterns/association_rules.py:186: RuntimeWarning: invalid value encountered in divide\n",
      "  cert_metric = np.where(certainty_denom == 0, 0, certainty_num / certainty_denom)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Your transaction data\n",
    "transactions_task2 = [\n",
    "    ['Shirt', 'Tie'],\n",
    "    ['Shirt', 'Belt', 'Tie'],\n",
    "    ['Shirt', 'Shoes', 'Tie'],\n",
    "    ['Tie', 'Belt', 'Shirt'],\n",
    "    ['Shirt', 'Tie', 'Belt']\n",
    "]\n",
    "\n",
    "# --- 1. Encode the transactions into a one-hot encoded DataFrame ---\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions_task2).transform(transactions_task2)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "print(\"--- One-Hot Encoded DataFrame ---\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# --- 2. Find frequent itemsets using the Apriori algorithm ---\n",
    "# You can adjust the 'min_support' threshold.\n",
    "# 'min_support=0.6' means an itemset must appear in at least 60% of transactions.\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "\n",
    "print(\"--- Frequent Itemsets (min_support=0.6) ---\")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# --- 3. Generate association rules from the frequent itemsets ---\n",
    "# You can adjust the 'min_threshold' and 'metric'.\n",
    "# 'metric=\"confidence\"' and 'min_threshold=0.7' means\n",
    "# A -> B is a rule if confidence(A -> B) >= 70%.\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "print(\"--- Association Rules (min_threshold=0.7, metric='confidence') ---\")\n",
    "print(rules)\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# --- Optional: Filter and sort rules for better insights ---\n",
    "print(\"--- Sorted Rules by Lift (descending) ---\")\n",
    "rules_sorted_by_lift = rules.sort_values(by=\"lift\", ascending=False)\n",
    "print(rules_sorted_by_lift)\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "print(\"--- Rules with High Confidence and Lift ---\")\n",
    "# Example: Rules where confidence is greater than 0.8 and lift is greater than 1.2\n",
    "high_confidence_lift_rules = rules[(rules['confidence'] > 0.8) & (rules['lift'] > 1.2)]\n",
    "print(high_confidence_lift_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Association Rules (Task 3 - Bookstore Purchases):\n",
      "  antecedents consequents  antecedent support  consequent support  support  \\\n",
      "0    (Book C)    (Book A)                 0.5                0.75      0.5   \n",
      "\n",
      "   confidence      lift  representativity  leverage  conviction  \\\n",
      "0         1.0  1.333333               1.0     0.125         inf   \n",
      "\n",
      "   zhangs_metric   jaccard  certainty  kulczynski  \n",
      "0            0.5  0.666667        1.0    0.833333  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "transactions_task3 = [\n",
    "    ['Book A', 'Book B'],\n",
    "    ['Book A', 'Book C'],\n",
    "    ['Book B', 'Book C', 'Book A'],\n",
    "    ['Book B', 'Book D']\n",
    "]\n",
    "\n",
    "te_task3 = TransactionEncoder()\n",
    "te_ary_task3 = te_task3.fit(transactions_task3).transform(transactions_task3)\n",
    "df_task3 = pd.DataFrame(te_ary_task3, columns=te_task3.columns_)\n",
    "\n",
    "# Identify frequent itemsets with a minimum support threshold of 40% (0.4)\n",
    "frequent_itemsets_task3 = apriori(df_task3, min_support=0.4, use_colnames=True)\n",
    "\n",
    "# Generate association rules with a confidence threshold of 70% (0.7)\n",
    "rules_task3 = association_rules(frequent_itemsets_task3, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "print(\"\\nAssociation Rules (Task 3 - Bookstore Purchases):\")\n",
    "print(rules_task3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
