{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title: Neural Networks\n",
    "\n",
    "# 1. Task 1: Digit Recognition with a Simple Neural Network\n",
    "# Task: Understand how a neural network identifies handwritten digits.\n",
    "# Exercise: Input an image of a handwritten digit into a pre-trained simple feedforward neural network and observe the output prediction.\n",
    "\n",
    "# 2. Task 2: Predicting House Prices\n",
    "# Task: Use a neural network to predict house prices based on features like location, size, and number of rooms.\n",
    "# Exercise: Experiment with different input data to observe how the network's predictions change.\n",
    "\n",
    "# 3. Task 3: Language Translation\n",
    "# Task: A neural network powers language translation tools to convert text from one language to another.\n",
    "# Exercise: Provide a sentence in English and get the translated version in another language using a pre-trained translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the digit recognition model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 16:58:14.308424: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Original image (flattened):\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]...\n",
      "True Digit: 5\n",
      "Predicted Digit: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Training the digit recognition model...\")\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "random_index = np.random.randint(0, len(x_test))\n",
    "sample_image = x_test[random_index]\n",
    "true_label = y_test[random_index]\n",
    "\n",
    "prediction = model.predict(sample_image.reshape(1, -1))\n",
    "predicted_digit = np.argmax(prediction)\n",
    "\n",
    "print(f\"\\nOriginal image (flattened):\\n{sample_image[:10]}...\")\n",
    "print(f\"True Digit: {true_label}\")\n",
    "print(f\"Predicted Digit: {predicted_digit}\")\n",
    "\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(sample_image.reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"Predicted: {predicted_digit}, True: {true_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the house price prediction model...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "data = {\n",
    "    'Size_sqft': np.random.randint(800, 3000, num_samples),\n",
    "    'Num_Bedrooms': np.random.randint(2, 6, num_samples),\n",
    "    'Num_Bathrooms': np.random.randint(1, 4, num_samples),\n",
    "    'Location_Score': np.random.uniform(1, 10, num_samples),\n",
    "    'Year_Built': np.random.randint(1950, 2020, num_samples)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['Price'] = (df['Size_sqft'] * 50) + \\\n",
    "              (df['Num_Bedrooms'] * 10000) + \\\n",
    "              (df['Num_Bathrooms'] * 7000) + \\\n",
    "              (df['Location_Score'] * 5000) + \\\n",
    "              ((2025 - df['Year_Built']) * -100) + \\\n",
    "              np.random.normal(0, 15000, num_samples)\n",
    "df['Price'] = np.maximum(50000, df['Price'])\n",
    "df.to_csv('house_data.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('house_data.csv')\n",
    "X = df.drop('Price', axis=1)\n",
    "y = df['Price']\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"\\nTraining the house price prediction model...\")\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.1)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "def predict_house_price(model, scaler_X, scaler_y, size, bedrooms, bathrooms, location_score, year_built):\n",
    "    input_data = np.array([[size, bedrooms, bathrooms, location_score, year_built]])\n",
    "    input_scaled = scaler_X.transform(input_data)\n",
    "    predicted_scaled = model.predict(input_scaled)[0][0]\n",
    "    predicted_price = scaler_y.inverse_transform([[predicted_scaled]])[0][0]\n",
    "    return predicted_price\n",
    "\n",
    "print(\"\\nExperimenting with house price predictions:\")\n",
    "price1 = predict_house_price(model, scaler_X, scaler_y, 1500, 3, 2, 7.5, 2000)\n",
    "print(f\"Predicted price for a 1500 sqft, 3 bed, 2 bath, loc 7.5, 2000 built house: ${price1:,.2f}\")\n",
    "\n",
    "price2 = predict_house_price(model, scaler_X, scaler_y, 2500, 4, 3, 9.0, 2010)\n",
    "print(f\"Predicted price for a 2500 sqft, 4 bed, 3 bath, loc 9.0, 2010 built house: ${price2:,.2f}\")\n",
    "\n",
    "price3 = predict_house_price(model, scaler_X, scaler_y, 1000, 2, 1, 3.0, 1970)\n",
    "print(f\"Predicted price for a 1000 sqft, 2 bed, 1 bath, loc 3.0, 1970 built house: ${price3:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_translate(sentence, target_language='Spanish'):\n",
    "    if target_language == 'Spanish':\n",
    "        translations = {\n",
    "            \"hello\": \"hola\",\n",
    "            \"world\": \"mundo\",\n",
    "            \"how are you\": \"cómo estás\",\n",
    "            \"i am fine\": \"estoy bien\",\n",
    "            \"thank you\": \"gracias\",\n",
    "            \"goodbye\": \"adiós\",\n",
    "            \"what is your name\": \"cómo te llamas\"\n",
    "        }\n",
    "        words = sentence.lower().split()\n",
    "        translated_words = []\n",
    "        for word in words:\n",
    "            found = False\n",
    "            for phrase, trans in translations.items():\n",
    "                if word in phrase.split():\n",
    "                    translated_words.append(trans if phrase == word else word)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                translated_words.append(word)\n",
    "        \n",
    "        # Simple phrase matching, not perfect for complex sentences\n",
    "        if \"how are you\" in sentence.lower():\n",
    "            return \"cómo estás\"\n",
    "        elif \"what is your name\" in sentence.lower():\n",
    "            return \"cómo te llamas\"\n",
    "        \n",
    "        return ' '.join(translated_words)\n",
    "    else:\n",
    "        return \"Translation for this language is not available in this simple model.\"\n",
    "\n",
    "print(\"\\n--- Language Translation (Conceptual Demonstration) ---\")\n",
    "english_sentence1 = \"Hello world\"\n",
    "translated_sentence1 = simple_translate(english_sentence1, 'Spanish')\n",
    "print(f\"English: '{english_sentence1}' -> Spanish (simple): '{translated_sentence1}'\")\n",
    "\n",
    "english_sentence2 = \"How are you\"\n",
    "translated_sentence2 = simple_translate(english_sentence2, 'Spanish')\n",
    "print(f\"English: '{english_sentence2}' -> Spanish (simple): '{translated_sentence2}'\")\n",
    "\n",
    "english_sentence3 = \"Thank you goodbye\"\n",
    "translated_sentence3 = simple_translate(english_sentence3, 'Spanish')\n",
    "print(f\"English: '{english_sentence3}' -> Spanish (simple): '{translated_sentence3}'\")\n",
    "\n",
    "print(\"\\n--- How a Real Neural Network (like a Transformer) Works for Translation ---\")\n",
    "print(\"1. Tokenization: Breaks down the input sentence into smaller units (words or sub-words).\")\n",
    "print(\"   Example: 'Hello world' -> ['Hello', 'world']\")\n",
    "print(\"2. Embedding: Converts tokens into numerical vectors that capture their semantic meaning.\")\n",
    "print(\"3. Encoder: Processes the input sequence, understanding context and relationships between words.\")\n",
    "print(\"   (Often uses attention mechanisms to weigh importance of different words.)\")\n",
    "print(\"4. Decoder: Generates the output sequence in the target language, attending to the encoded input and previously generated words.\")\n",
    "print(\"5. Output Layer: Maps the decoder's output to probabilities for each word in the target language's vocabulary, selecting the most probable word.\")\n",
    "print(\"This process iterates until an 'end of sentence' token is generated.\")\n",
    "print(\"\\nTo truly implement this, you would use libraries like Hugging Face Transformers to load pre-trained models such as 'Helsinki-NLP/opus-mt-en-es' for English to Spanish.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
